{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series, DataFrame\n",
    "import pytz\n",
    "from pytz import common_timezones, all_timezones\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chapter 0 Preface\n",
    "\n",
    "This book is an introduction to the practical tools of exploratory data analysis. The organization of the book follows the process I use when I start working with a dataset:\n",
    "\n",
    "* Importing and cleaning: Whatever format the data is in, it usually takes some time and effort to read the data, clean and transform it, and check that everything made it through the translation process intact.\n",
    "* Single variable explorations: I usually start by examining one variable at a time, finding out what the variables mean, looking at distributions of the values, and choosing apprpriate summary statistics\n",
    "* Pair-wise explorations: To identify possible relationships between variables, I look at tables and scatter plots, and compute correlations and linear fits\n",
    "* Multivariate analysis: If there are apparent relationships between variables, I use multiple regression to add control variables and investigate more complex relationships\n",
    "* Estimation and hypothesis testing: When reporting statistical results, it is important to answer three questions: How big is the effect? How much variability should we expect if we run the same measurement again? Is it possible that the apparent effect is due to chance?\n",
    "* Visualization: During exploration, visualization is an important tool for finding possible relationships and effects. Then if an apparent effect holds up to scrutiny, visualization is an effective way to communicate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "http://nbviewer.jupyter.org/github/barbagroup/AeroPython/blob/master/lessons/01_Lesson01_sourceSink.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#from IPython.core.display import HTML\n",
    "#def css_styling():\n",
    "#    styles = open('../styles/custom.css', 'r').read()\n",
    "#    return HTML(styles)\n",
    "#css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This book takes a comuational approach, which has several advantages over mathematical approaches:\n",
    "\n",
    "* I present most ideas using Python code, rather than mathematical notation. In general, Python code is more readable; also, because it is executable, readers can download it, run it, and modify it.\n",
    "* Each chapter includes exercises readers can do to develop and solidify their learning. When you write programs, you express your understanding in code; while you are debugging the program, you are also correcting your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Some exercises involve experiments to test statistical behavior. For example, you can explore the Central Limit Theorem (CLT) by generating random samples and computing their sums. The resulting visiualizations demonstrate why the CLT works and and when it doesn't.\n",
    "* Some ideas that are hard to grasp mathematically are easy to understand by simulation. For example, we approximate p-values by running random simulations, which reinforces the meaning of the p-value.\n",
    "* Because the book is based on a general-purpose programming language (Python), readers can import data from almost any source. They are not limited to datasets that have been cleaned and formatted for a partciular statistics tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The book lends itself to a project-based approach. In my class, students work on a semester-long project that requires them to pose a statistical question, find a dataset that can address it, and apply each of the techniques they learn to their own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To demonstrate my approach to statistical analysis, the book presents a case study that runs through all of the chapters. It uses data from two sources:\n",
    "\n",
    "* The Natoinal Survey of Family Growth (NSFG), conducted by the U.S. Centers for Disease Control and Prevention (CDC) to gather \"information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men's and women's health.\" (See http://cdc/gov/nchs/nsfg.html)\n",
    "* The Behavioral Risk Factor Surveillance System (BRFSS), conducted by the National Center for Chronic Disease and Prevention and Health Promotion to \"track conditions and risk behaviors in the United States.\" (See http://cdc.gov/BRFSS/)\n",
    "\n",
    "Other examples use data from the IRS, the U.S. Census, and the Boston Marathon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This second edition of _Think Stats_ includes the chapters from the first edition, many of them substantially revised, and new chapters on regression, time series analysis, survival analysis, and analytic methods. The previous edition did not use pandas, SciPy, or StatsModels, so all of that material is new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#from IPython.core.display import HTML\n",
    "#def css_styling():\n",
    "#    styles = open('theme/serif.css', 'r').read()\n",
    "#    return HTML(styles)\n",
    "#css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='teal'> 0.1 How I wrote this book</font>\n",
    "\n",
    "When people write a new textbook, they usually start by reading a stack of old textbooks. As a result, most books contain the same material in pretty much the same order.\n",
    "\n",
    "I did not do that. In fact, I used almost no printed material while I was writing this book, for several reasons:\n",
    "\n",
    "* My goal was to explore a new approach to this material, so I didn't want much exposure to existing approaches.\n",
    "* Since I am making this book available under a free license, I wanted to make sure that no part of it was encumbered by copyright restrictions.\n",
    "* Many readers of my books don't ahve access to libraries of printed material, so I triedto make references to resources that are freely available on the internet.\n",
    "* Some proponents of old media think that the exclusive use of electronic resources is lazy and unreliable. They might be right about the first part, but I think they are wrong about the second, so I wanted to test my theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The resource I used more than any other is Wikipedia. In general, the articles I read on statistical topics were very good (although I made a few small changes along the way). I  include references to Wikipedia pages throughout the book and I encourage you to follow those links; in many cases, the Wikipedia page picks up where my description leaves off. The vocabulary and notation in this book are generally consistent with Wikipedia, unless I had a good reason to deviate. Other resources I found useful were Wolfram MathWorld and the Reddit statistics forum, http://www.reddit.com/r/statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='teal'>0.2 Using the code</font>\n",
    "\n",
    "The code and data used in this book are available from https://github/com/AllenDowney/ThinkStats2. Git is a veresoin control system that allows you to keep track of the files that make up a project. A Collection of files under Git's control is called a **repository**. GitHub is a hosting service that provides storage for Git repositories and a convenient web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The GitHub homepage for my repository provides several ways to work with the code:\n",
    "\n",
    "* You can create a copy of my repository on GitHub by pressing the Fork button. If you don't already have a GitHub account, you'll need to create one. After forking, you'll have your own repository on GitHub that you can use to keep track of code you write while working on this book. Then you can clone the repo, which means that you make a copy of hte files on your computer.\n",
    "* Or you could clone my reopsitory. You don't need a GitHub account to do this, but you won't be able to write any changes back to GitHub.\n",
    "* If you don't want to use Git at all, you can download the files in a Zip file using the button in the lower-right corner of the GitHub page.\n",
    "\n",
    "All of the code is written to work in both Python 2 and Python 3 with no translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I developed this book using Anaconda from Continuum Analytics, which is a free Python distribution tha tincludes all the packages you'll need to run the code (and lots more). I found Anaconda easy to install. By default it does a user-level installaiton, not a system-level, so you don't need administrative privileges. And it supports both Python 2 and Python 3. You can download Anaconda from http://conintuum.io/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you don't want to use Anaconda, you will need the following packages:\n",
    "\n",
    "* pandas for representing and analyzing data, http://pandas.pydata.org\n",
    "* NumPy for basic numerical computation, http://www.numpy.org\n",
    "* SciPy for scientific computation including statistics, http://www.scipy.org\n",
    "* StatsModels for regression and other statistical analysis, http://statsmodels.sourceforge.net ; and\n",
    "* matplotlib for visualization, http://matplotlib.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Although these are commonly used packages, they are not included with all Python installations, and they can be hard to install in some environments. If you are having trouble installing them, I strongly recommend using Anaconda or one of the other Python distributions that include these packages.\n",
    "\n",
    "After you clone the repository or unzip the zip file, you should have a folder called ```ThinkStats2/code``` with a file called ```nsfg.py```. If you run nsfg.py, it should read a data file, run some tests, and print a message like, \"All tests passed.\" If you get import errors, it probably means there are packages you need to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13593, 244)\n",
      "nsfg.py: All tests passed.\n"
     ]
    }
   ],
   "source": [
    "%run nsfg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Most exercises use Python scripts, but some also use the IPython notebook. If you have not used IPython notebook before, I suggest you start with the documentation at http://ipython.org/ipython-doc/stable/notebook/notebook.html\n",
    "\n",
    "I wrote this book assuming that the reader is familiar with core Python, including object-oriented features, but not pandas, NumPy, and SciPy. If you are already familiar with these modules, you can skip a few sections.\n",
    "\n",
    "I assume that the reader knows basic mathematics, including logarithms, for example, and summations. I refer to calculus concepts in a few places, but you don't have to do any calculus.\n",
    "\n",
    "If you have never studied statistics, I think this book is a good place to start. And if you have taken a traditional statistics class, I hope this book will help repaair the damage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='teal'>Chapter 1 Exploratory data Analysis</font>\n",
    "\n",
    "The thesis of this book is that data combined with practical methods can answer questions and guide decisions under uncertainty.\n",
    "\n",
    "As an example, I present a case study motivated by a question I heard when my wife and I were expecting our first child: do first babies tend to arrive late?\n",
    "\n",
    "If you Google this question, you will find plenty of discussion. Some people claim it's true, others say it's a myth, and some people say it's the other way around: first babies come early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In many of these discussions, people provide data to support their claims. I found many examples like these:\n",
    "\n",
    "* \"My two friends that have given birth recently to their first babies, BOTH went almost 2 weeks overdue before going into labour or being induced.\"\n",
    "* \"My first one came 2 weeks late and now I think the second one is going to come out two weeks early!!\"\n",
    "* \"I don't think that can be true because my sister was my mother's first and she was early, as with many of my cousins.\"\n",
    "\n",
    "\n",
    "Reports like these are called **anecdotal evidence** because they are based on data that is unpublished and usually personal. In casaul conversation, there is nothing wrong with anecdotes, so I don't mean to pick on the people I quoted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we might want evidence that is more persuasive and an answer that is more realiable. By those standards, anecdotal evidence usually fails, because:\n",
    "\n",
    "* Small number of observations: If pregnancy length is longer for first babies, the difference is probably small compared to natural variation. In that case, we might have to compare a large number of pregnancies to be sure that a difference exists.\n",
    "* Selection bias: People who join a discussion of this question might be interested because their first babies were late. In that case the process of selecting data would bias the results.\n",
    "* Confirmation bias: People who believe the claim might be more likely to contribute examples that confirm it. People who doubt the claim are more likely to cite counterexamples.\n",
    "* Inaccuracy: Anecdotes are often personal stories, and oftem misremembered, misrepresetned, repeated inaccurately, etc.\n",
    "\n",
    "So how can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='teal'>1.1 A statistical approach</font>\n",
    "\n",
    "To address the limitations of anecdotes, we will use the tools of statistics, which include:\n",
    "\n",
    "* Data collection: We will use data from a large national survey that was designed explicitly with the goal of generating statistically valid inferences about the U.S. population.\n",
    "* Descriptive statistics: We will generate statistics that summarize the data concisely, and evluate different ways to visualize data.\n",
    "* Exploratory data analysis: We will look for patterns, differences, and other features that address the questions we are interested in. At the same time we will check for inconsistencies and identify limitations.\n",
    "* Estimation: We will use data from a sample to estimate characteristics of the general population.\n",
    "* Hypothesis testing: Where we see apparent effects, like a difference between two groups, we will evaluate whether the effect might have happened by chance.\n",
    "\n",
    "By performing these steps with care to avoid pitfalls, we can reach conclusions that are more justifiable and more likely to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
